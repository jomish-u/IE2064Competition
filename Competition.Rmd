---
title: "Competition"
author: "Joshua Misiura"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
#Bring in Libraries
library(readr)
library(readxl)
library(tidyverse)
library(caret)
library(corrplot)
```

Note for below: removing the check for 0s and highly correlated predictors reduces model effectiveness for training set. Approximate difference is .16 (RMSE for no predictor removal was 2.86 vs 2.71 for the removal of predictors)

```{r echo=FALSE, message=FALSE, warning=FALSE}
#load data
training <- read.csv("competition-data.csv") 
test_preds <- read.csv("competition-test-x-values.csv") 
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
#cleaning and pred selection
zero_check <- training %>% summarise(across(everything(), ~ sum(. == 0)))
#Predictors which have over 1,000 0's are being automatically removed. We can't assess the signifigance of individual columns, so this is the best route to retain predictors which have unique values
training <- training %>% select(X2:X10, X17:outcome, - X8)
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
## Removing X4, X5, X6, X17 for highly correlated, X7 for low var
training <- training %>% select(- (X4:X6),- X17)
##correlation_matrix <- cor(training)
##pairs(training)
```
```{r}
x <- training %>% select(-outcome)
y <- training$outcome
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
# THIS IS THE SELECTED MODEL
rfFit <- train( x = x,
                y = y,
                preProcess = c("center", "scale", "BoxCox"),
                method="rf", 
                trControl=ctrl
          )

```

```{r echo=FALSE, message=FALSE, warning=FALSE}
rf_predictions <- predict(rfFit, newdata = test_preds)
write.csv(rf_predictions, file = "predictions.csv", row.names = FALSE)

```

